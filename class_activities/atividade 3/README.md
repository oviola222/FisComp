# üìÑ `FNCC para treinamento de fun√ß√µes via Tensor Flow`

Este projeto demonstra como construir, treinar e avaliar uma FNCC utilizando utilizando os pacotes **scikit-learn** e o **TensorFlow**, para treinar a aproxima√ß√£o de algumas fun√ß√µes. Inicialmente, em `atividade3_sklearn.ipynb`, foi implementado o treinamento para as fun√ß√µes $\sin x$, ${\sin x}/x$ e $e^{-x^2}$ utilizando o scikit-learn em sala de aula. Logo, para essa atividade 3, vamos realizar uma implementa√ß√£o semelhante s√≥ que no TensorFlor.

Sendo assim, este exemplo inicial visa explicar passo a passo a implementa√ß√£o do c√≥digo para o exemplo da fun√ß√£o fun√ß√£o $f(x) = \sin x$, que est√° no arquivo `atividade3_tensorflow.ipynb`. 

---

## 1. Importa√ß√£o das Bibliotecas

```python
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
```

---

## 2. Gera√ß√£o dos Dados de Treinamento

```python
np.random.seed(42)
num_samples = 100
data_train = np.random.uniform(0, 2 * np.pi, num_samples).reshape(-1, 1)
sin_values_train = np.sin(data_train)

noise = np.random.normal(0, 0.1, sin_values_train.shape)
sin_values_train += noise
```

- Gera 200 pontos aleat√≥rios entre 0 a $2\pi$ com a seed 42.
- Calcula a fun√ß√£o $f(x)=\sin x$ para cada ponto.
- Adiciona ru√≠do (noise) de $\pm0.1$ em $f(x)$ para simular um cen√°rio mais realista.

---

## 3. Defini√ß√£o do Modelo FCNN

```python
model = tf.keras.Sequential([
    tf.keras.layers.Dense(5, activation='tanh', input_shape=(1,)),
    tf.keras.layers.Dense(5, activation='tanh'),
    tf.keras.layers.Dense(1)  # sa√≠da cont√≠nua (linear)
])
```

- Cria a rede neural sequencial com 2 camadas ocultas (5 neur√¥nios cada) utilizando o Keras
- Keras √© uma Interface de Programa√ß√£o de Aplica√ß√µes implementada pelo TensorFlow para criar a estrutura da rede neural, aplicando os pesos, bias e fun√ß√µes de ativa√ß√£o.
- Nesse caso, a fun√ß√£o de ativa√ß√£o escolhida √© a `tanh`, pois o dom√≠nio √© o mesmo que a da fun√ß√£o de treinamento.
- O `input_shape` especifica o formato dos dados de entrada: no caso √© 1D, uma lista de valores de $x$.
- Por fim, como essa rede neural retorna um valor de sa√≠da, um n√∫mero real cont√≠nuo (a predi√ß√£o para a fun√ß√£o de treinamento), essa sa√≠da deve ter apenas **1 neur√¥nio**, sem ativa√ß√£o (linear).

---

## 4. Compila√ß√£o e Treinamento

```python
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),
              loss='mse')

model.fit(data_train, sin_values_train, epochs=500, verbose=0)
```

- Otimizador: Adam (gradiente descendente aprimorado) com taxa de aprendizado $\alpha$ de 0.01.
- Fun√ß√£o de perda: MSE (erro quadr√°tico m√©dio).
- Treinamento por 500 √©pocas (quantidade de itera√ß√µes).
- `verbose=0` n√£o mostra a barra de progresso com informa√ß√µes sobre cada epoch de treinamento, incluindo a perda (loss) e o tempo.

---

## 5. Gera√ß√£o dos Dados de Teste

```python
num_test_samples = 100
data_test = np.linspace(0, 4 * np.pi, num_test_samples).reshape(-1, 1)
sin_values_true = np.sin(data_test)
```

- Gera 100 pontos de teste igualmente espa√ßados no intervalo $[0, 4\pi]$ para a rede neural prever a fun√ß√£o al√©m dos pontos de treinamento.

---

## 6. Previs√µes e Avalia√ß√£o

```python
sin_values_predicted = model.predict(data_test)
mse = np.mean(np.square(sin_values_true - sin_values_predicted))
print(f"Mean Squared Error on Test Data: {mse}")
```

- Faz previs√µes com o modelo treinado.
- Calcula o erro quadr√°tico m√©dio manualmente com NumPy.

---

## 7. Visualiza√ß√£o dos Resultados

```python
plt.figure(figsize=(6, 5))
plt.scatter(data_train, sin_values_train, label='Training Data', alpha=0.5)
plt.plot(data_test, sin_values_true, label=r'f(x)=\sin x$', color='blue')
plt.plot(data_test, sin_values_predicted, label=r'Predicted $\sin x$', color='red')
plt.xlabel(r'$x$')
plt.ylabel(r'$\sin x$')
plt.title(r'Aproxima√ß√£o de $\sin x$ com FCNN (TensorFlow)')
plt.legend(fontsize=12)
plt.grid(True)
plt.show()
```

- Plota os dados de treino com ru√≠do, a curva real e a curva predita.
- Visualmente, permite avaliar se o modelo aprendeu bem a fun√ß√£o.

---

## ‚úÖ Conclus√£o

Com apenas TensorFlow e NumPy, conseguimos treinar uma rede neural simples para aproximar uma fun√ß√£o matem√°tica suave. Mesmo com ru√≠do, o modelo √© capaz de aprender a estrutura subjacente da fun√ß√£o $f(x) = \sin x$, mostrando o eficiencia das redes neurais em tarefas de regress√£o num cen√°rio realista.